{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from components.mymodel import load_model, Models\n",
    "from components.helper import train\n",
    "from components.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "image_set = Imageset.p\n",
    "device = Devices.all\n",
    "environment = Environments.all\n",
    "model_name = Models.alexnet\n",
    "\n",
    "params:dict = dict({\n",
    "    'epochs': 200,\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 200,\n",
    "    'Imageset': image_set.value,\n",
    "    'Device': device.value,\n",
    "    'Environment': environment.value,\n",
    "    'model_name': model_name.value,\n",
    "    'clip_target': True\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"https://web-mlflow.akraradets.duckdns.org\")\n",
    "mlflow.set_experiment(experiment_name='Soil')\n",
    "mlflow.start_run()\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset:SoilDataset_bigset, epochs:int, lr:float, batch_size:int):\n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=30)\n",
    "    model, train_losses = train(model, loader, epochs, lr, DEVICE)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title(dataset.signature)\n",
    "    plt.show()\n",
    "    return model, train_losses\n",
    "\n",
    "#We can check whether we have gpu\n",
    "DEVICE = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "# DEVICE = 'cpu'\n",
    "print(\"Device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# img = Image.open(dataset.imgs[100])\n",
    "# # dataset.imgs[100]\n",
    "# img\n",
    "# plt.imshow( img_array )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(350),\n",
    "    transforms.CenterCrop(224),\n",
    "    # transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "dataset = SoilDataset_bigset(imageset=image_set, device=device, environment=environment, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_name=model_name)\n",
    "model, train_loss = train_model(model, dataset=dataset, epochs=params['epochs'], lr=params['lr'], batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model.to('cpu')\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(350),\n",
    "    transforms.CenterCrop(224),\n",
    "    # transforms.RandomCrop(224),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "dataset = SoilDataset_bigset(imageset=image_set, device=device, environment=environment, transform=preprocess)\n",
    "loader = DataLoader(dataset=dataset, batch_size=params['batch_size'], shuffle=False, num_workers=1)\n",
    "ys = []\n",
    "yhats = []\n",
    "pic_names = []\n",
    "with torch.no_grad():\n",
    "    for X,y,pic_name in tqdm(loader):\n",
    "        yhat = model(X)\n",
    "        pic_names.append(list(pic_name))\n",
    "        ys.append(y.reshape(-1))\n",
    "        yhats.append(yhat.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([np.hstack(ys), np.hstack(yhats), np.hstack(pic_names)]).T\n",
    "df.rename(columns = {\n",
    "    0:'Target',\n",
    "    1:'Predict',\n",
    "    2:'Image name'\n",
    "}, inplace=True)\n",
    "df.set_index('Image name', inplace=True)\n",
    "artifact_name:str = os.path.join('artifact','inference.csv')\n",
    "df.to_csv(artifact_name)\n",
    "mlflow.log_artifact(artifact_name)\n",
    "mlflow.end_run()\n",
    "os.remove(artifact_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
