{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import io\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageFont, ImageDraw \n",
    "\n",
    "from components.dataset import SoilDataset, SoilDataset_phone\n",
    "from components.mymodel import get_model\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/30 08:00:28 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - mlflow (current: 2.4.1, required: mlflow==2.4)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"https://web-mlflow.akraradets.duckdns.org\")\n",
    "logged_model = 'runs:/286009600e9a41859b472a34ead09619/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import io\n",
    "from enum import Enum\n",
    "\n",
    "class Devices(Enum):\n",
    "    iphone = 'Iphone'\n",
    "    oppo = 'Oppo'\n",
    "    redmi = 'Redmi'\n",
    "    samsung = 'Samsung'\n",
    "    all = '*'\n",
    "\n",
    "class Environments(Enum):\n",
    "    indoor = 'Indoor'\n",
    "    outdoor = 'Outdoor'\n",
    "    all = '*'\n",
    "\n",
    "class Imageset(Enum):\n",
    "    om = 'OM'\n",
    "    p = 'P'\n",
    "\n",
    "class SoilDataset_bigset(Dataset):\n",
    "    def __init__(self, imageset:Imageset, device:Devices, environment:Environments, transform=None):\n",
    "        # BasePath of the dataset\n",
    "        dataset_path:str = './dataset/bigset/'\n",
    "        assert os.path.exists(dataset_path), f\"{dataset_path=} is not exist.\"\n",
    "        # Inside this path there must be a list of folders arange by mobile phone. Use device enum.\n",
    "        # Inside those mobile phone are 2 folders indicate the environment the image was taken in. Use environment enum.\n",
    "        image_folder = os.path.join(dataset_path, imageset.value, device.value, environment.value)\n",
    "        self.imgs = glob(os.path.join(image_folder,'*/*'))\n",
    "        print(f\"Found {len(self.imgs)} images in {image_folder}.\")\n",
    "\n",
    "        # Load csv file for lookup the target value\n",
    "        target_path:str = os.path.join(dataset_path,imageset.value,'meta.csv')\n",
    "        self.target_df = pd.read_csv(target_path, index_col='id')\n",
    "\n",
    "        self.signature = os.path.join(imageset.value,device.value,environment.value)\n",
    "        self.transform = transform\n",
    "\n",
    "    def get_target(self, img_path:str) -> float:\n",
    "        assert len(img_path.split('/')) == 8, f\"Expect img_path to have 8 folders but got {img_path=}\"\n",
    "        target_id = int(img_path.split('/')[6])\n",
    "        return self.target_df.loc[target_id] # type:ignore\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        y = self.get_target(img_path=img_path)\n",
    "        y = torch.tensor(y)\n",
    "        X = io.read_image(img_path)\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "        return X.float(), y.float(), img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(350),\n",
    "    transforms.CenterCrop(224),\n",
    "    # transforms.RandomCrop(224),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "dataset = SoilDataset_bigset(imageset=Imageset.om, device=Devices.all, environment=Environments.all, transform=preprocess)\n",
    "loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "for X,y,pic_name in tqdm(loader):\n",
    "    yhat = loaded_model(X).detach()[0][0]\n",
    "    img = Image.open(f'./dataset/phones/{image_set}/{pic_name[0]}')\n",
    "    y = y[0]\n",
    "    pic_name = os.path.join(result_path,pic_name[0])\n",
    "    save_result(img, y.numpy(), yhat.numpy(), pic_name)\n",
    "\n",
    "    _,pic_name = os.path.split(pic_name)\n",
    "    pic_names.append(pic_name)\n",
    "    ys.append(float(y))\n",
    "    yhats.append(float(yhat))\n",
    "    # break\n",
    "return pic_names, ys, yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(img:Image.Image, label:str, predict:str, picname:str):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.load_default()\n",
    "    # font.\n",
    "    import cv2\n",
    "    font_size = 30\n",
    "    font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
    "    font = ImageFont.truetype(font_path, size=font_size)\n",
    "    draw.text((10, 10),f\"Label:{label}\",(0,0,0), font=font)\n",
    "    draw.text((10, 40 + font_size),f\"Predict:{predict}\",(0,0,0), font=font)\n",
    "    img.save(picname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_result(model_name:str, image_set:str):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(350),\n",
    "        transforms.CenterCrop(224),\n",
    "        # transforms.RandomCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    model = get_model(model_name=model_name, image_set=image_set)\n",
    "    dataset = SoilDataset_phone('./dataset/phones', image_set, transform=preprocess)\n",
    "    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "    # toPIL = transforms.transforms.ToPILImage()\n",
    "    result_path = os.path.join('./result/',model_name,image_set)\n",
    "    if(os.path.exists(result_path) == False):\n",
    "        os.makedirs(result_path)\n",
    "\n",
    "    pic_names = []\n",
    "    ys = []\n",
    "    yhats = []\n",
    "    for X,y,pic_name in tqdm(loader):\n",
    "        yhat = model(X).detach()[0][0]\n",
    "        img = Image.open(f'./dataset/phones/{image_set}/{pic_name[0]}')\n",
    "        y = y[0]\n",
    "        pic_name = os.path.join(result_path,pic_name[0])\n",
    "        save_result(img, y.numpy(), yhat.numpy(), pic_name)\n",
    "\n",
    "        _,pic_name = os.path.split(pic_name)\n",
    "        pic_names.append(pic_name)\n",
    "        ys.append(float(y))\n",
    "        yhats.append(float(yhat))\n",
    "        # break\n",
    "    return pic_names, ys, yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2355 images in ./dataset/phones/All.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cabacf264b4b4184a118d5245c01b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_set_list = next(os.walk('./dataset/phones/'))[1]\n",
    "# mobilenet_v3_large\n",
    "# resnet50\n",
    "# efficientnet_v2_l\n",
    "# alexnet\n",
    "model_name = 'alexnet'\n",
    "# for image_set in image_set_list:\n",
    "pic_names, ys, yhats = run_result(model_name,'All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['pic_name'] = pic_names\n",
    "df['OM'] = ys\n",
    "df['predict'] = yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pic_name</th>\n",
       "      <th>OM</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_1.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.994197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_2.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.962518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_3.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.283316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_4.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.128171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_5.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.201982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_1.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.234501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_2.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.961864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_3.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.477397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_4.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.993420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_5.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.069716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2355 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           pic_name    OM   predict\n",
       "0        (1)_A_0.27_65S-1_C_R_1.JPG  0.27  1.994197\n",
       "1        (1)_A_0.27_65S-1_C_R_2.JPG  0.27  0.962518\n",
       "2        (1)_A_0.27_65S-1_C_R_3.JPG  0.27  1.283316\n",
       "3        (1)_A_0.27_65S-1_C_R_4.JPG  0.27  1.128171\n",
       "4        (1)_A_0.27_65S-1_C_R_5.JPG  0.27  1.201982\n",
       "...                             ...   ...       ...\n",
       "2350  (99)_S_0.86_65S-197_M_R_1.jpg  0.86  1.234501\n",
       "2351  (99)_S_0.86_65S-197_M_R_2.jpg  0.86  0.961864\n",
       "2352  (99)_S_0.86_65S-197_M_R_3.jpg  0.86  1.477397\n",
       "2353  (99)_S_0.86_65S-197_M_R_4.jpg  0.86  0.993420\n",
       "2354  (99)_S_0.86_65S-197_M_R_5.jpg  0.86  1.069716\n",
       "\n",
       "[2355 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic_names_new = []\n",
    "# for name in pic_names:\n",
    "#     _,pic_name = os.path.split(name)\n",
    "#     pic_names_new.append(pic_name)\n",
    "\n",
    "# y_new = []\n",
    "# for y in ys:\n",
    "#     y_new.append(float(y))\n",
    "\n",
    "# yhat_new = []\n",
    "# for yhat in yhats:\n",
    "#     yhat_new.append(float(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new = pd.DataFrame()\n",
    "# df_new['pic_name'] = pic_names_new\n",
    "# df_new['OM'] = y_new\n",
    "# df_new['predict'] = yhat_new\n",
    "# df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new.to_csv('result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
