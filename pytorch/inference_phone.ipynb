{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import io\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageFont, ImageDraw \n",
    "\n",
    "from components.dataset import SoilDataset, SoilDataset_phone\n",
    "from components.mymodel import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(img:Image.Image, label:str, predict:str, picname:str):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.load_default()\n",
    "    # font.\n",
    "    import cv2\n",
    "    font_size = 30\n",
    "    font_path = os.path.join(cv2.__path__[0],'qt','fonts','DejaVuSans.ttf')\n",
    "    font = ImageFont.truetype(font_path, size=font_size)\n",
    "    draw.text((10, 10),f\"Label:{label}\",(0,0,0), font=font)\n",
    "    draw.text((10, 40 + font_size),f\"Predict:{predict}\",(0,0,0), font=font)\n",
    "    img.save(picname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_result(model_name:str, image_set:str):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        # transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    model = get_model(model_name=model_name, image_set=image_set)\n",
    "    dataset = SoilDataset_phone('./dataset/phones', image_set, transform=preprocess)\n",
    "    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "    # toPIL = transforms.transforms.ToPILImage()\n",
    "    result_path = os.path.join('./result/',model_name,image_set)\n",
    "    if(os.path.exists(result_path) == False):\n",
    "        os.makedirs(result_path)\n",
    "\n",
    "    pic_names = []\n",
    "    ys = []\n",
    "    yhats = []\n",
    "    for X,y,pic_name in loader:\n",
    "        yhat = model(X).detach()[0][0]\n",
    "        img = Image.open(f'./dataset/phones/{image_set}/{pic_name[0]}')\n",
    "        y = y[0]\n",
    "        pic_name = os.path.join(result_path,pic_name[0])\n",
    "        save_result(img, y.numpy(), yhat.numpy(), pic_name)\n",
    "\n",
    "        _,pic_name = os.path.split(pic_name)\n",
    "        pic_names.append(pic_name)\n",
    "        ys.append(float(y))\n",
    "        yhats.append(float(yhat))\n",
    "        # break\n",
    "    return pic_names, ys, yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2355 images in ./dataset/phones/All.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmobilenet_v3_large\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m# for image_set in image_set_list:\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m pic_names, ys, yhats \u001b[39m=\u001b[39m run_result(model_name,\u001b[39m'\u001b[39;49m\u001b[39mAll\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mrun_result\u001b[0;34m(model_name, image_set)\u001b[0m\n\u001b[1;32m     21\u001b[0m ys \u001b[39m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m yhats \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfor\u001b[39;00m X,y,pic_name \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     24\u001b[0m     yhat \u001b[39m=\u001b[39m model(X)\u001b[39m.\u001b[39mdetach()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./dataset/phones/\u001b[39m\u001b[39m{\u001b[39;00mimage_set\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mpic_name[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_set_list = next(os.walk('./dataset/phones/'))[1]\n",
    "# mobilenet_v3_large\n",
    "# resnet50\n",
    "# efficientnet_v2_l\n",
    "# alexnet\n",
    "model_name = 'mobilenet_v3_large'\n",
    "# for image_set in image_set_list:\n",
    "pic_names, ys, yhats = run_result(model_name,'All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['pic_name'] = pic_names\n",
    "df['OM'] = ys\n",
    "df['predict'] = yhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pic_name</th>\n",
       "      <th>OM</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./result/alexnet/All/(1)_A_0.27_65S-1_C_R_1.JPG</td>\n",
       "      <td>tensor(0.2700)</td>\n",
       "      <td>tensor(0.3317)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./result/alexnet/All/(1)_A_0.27_65S-1_C_R_2.JPG</td>\n",
       "      <td>tensor(0.2700)</td>\n",
       "      <td>tensor(0.4124)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./result/alexnet/All/(1)_A_0.27_65S-1_C_R_3.JPG</td>\n",
       "      <td>tensor(0.2700)</td>\n",
       "      <td>tensor(0.4200)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./result/alexnet/All/(1)_A_0.27_65S-1_C_R_4.JPG</td>\n",
       "      <td>tensor(0.2700)</td>\n",
       "      <td>tensor(0.3021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./result/alexnet/All/(1)_A_0.27_65S-1_C_R_5.JPG</td>\n",
       "      <td>tensor(0.2700)</td>\n",
       "      <td>tensor(0.3334)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>./result/alexnet/All/(99)_S_0.86_65S-197_M_R_1...</td>\n",
       "      <td>tensor(0.8600)</td>\n",
       "      <td>tensor(0.5086)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>./result/alexnet/All/(99)_S_0.86_65S-197_M_R_2...</td>\n",
       "      <td>tensor(0.8600)</td>\n",
       "      <td>tensor(0.6553)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>./result/alexnet/All/(99)_S_0.86_65S-197_M_R_3...</td>\n",
       "      <td>tensor(0.8600)</td>\n",
       "      <td>tensor(0.6624)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>./result/alexnet/All/(99)_S_0.86_65S-197_M_R_4...</td>\n",
       "      <td>tensor(0.8600)</td>\n",
       "      <td>tensor(0.6350)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>./result/alexnet/All/(99)_S_0.86_65S-197_M_R_5...</td>\n",
       "      <td>tensor(0.8600)</td>\n",
       "      <td>tensor(0.5944)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2355 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               pic_name              OM  \\\n",
       "0       ./result/alexnet/All/(1)_A_0.27_65S-1_C_R_1.JPG  tensor(0.2700)   \n",
       "1       ./result/alexnet/All/(1)_A_0.27_65S-1_C_R_2.JPG  tensor(0.2700)   \n",
       "2       ./result/alexnet/All/(1)_A_0.27_65S-1_C_R_3.JPG  tensor(0.2700)   \n",
       "3       ./result/alexnet/All/(1)_A_0.27_65S-1_C_R_4.JPG  tensor(0.2700)   \n",
       "4       ./result/alexnet/All/(1)_A_0.27_65S-1_C_R_5.JPG  tensor(0.2700)   \n",
       "...                                                 ...             ...   \n",
       "2350  ./result/alexnet/All/(99)_S_0.86_65S-197_M_R_1...  tensor(0.8600)   \n",
       "2351  ./result/alexnet/All/(99)_S_0.86_65S-197_M_R_2...  tensor(0.8600)   \n",
       "2352  ./result/alexnet/All/(99)_S_0.86_65S-197_M_R_3...  tensor(0.8600)   \n",
       "2353  ./result/alexnet/All/(99)_S_0.86_65S-197_M_R_4...  tensor(0.8600)   \n",
       "2354  ./result/alexnet/All/(99)_S_0.86_65S-197_M_R_5...  tensor(0.8600)   \n",
       "\n",
       "             predict  \n",
       "0     tensor(0.3317)  \n",
       "1     tensor(0.4124)  \n",
       "2     tensor(0.4200)  \n",
       "3     tensor(0.3021)  \n",
       "4     tensor(0.3334)  \n",
       "...              ...  \n",
       "2350  tensor(0.5086)  \n",
       "2351  tensor(0.6553)  \n",
       "2352  tensor(0.6624)  \n",
       "2353  tensor(0.6350)  \n",
       "2354  tensor(0.5944)  \n",
       "\n",
       "[2355 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic_names_new = []\n",
    "# for name in pic_names:\n",
    "#     _,pic_name = os.path.split(name)\n",
    "#     pic_names_new.append(pic_name)\n",
    "\n",
    "# y_new = []\n",
    "# for y in ys:\n",
    "#     y_new.append(float(y))\n",
    "\n",
    "# yhat_new = []\n",
    "# for yhat in yhats:\n",
    "#     yhat_new.append(float(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pic_name</th>\n",
       "      <th>OM</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_1.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.331668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_2.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.412420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_3.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.420040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_4.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.302122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1)_A_0.27_65S-1_C_R_5.JPG</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.333441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_1.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.508597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_2.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.655261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_3.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.662441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_4.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.635019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>(99)_S_0.86_65S-197_M_R_5.jpg</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.594375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2355 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           pic_name    OM   predict\n",
       "0        (1)_A_0.27_65S-1_C_R_1.JPG  0.27  0.331668\n",
       "1        (1)_A_0.27_65S-1_C_R_2.JPG  0.27  0.412420\n",
       "2        (1)_A_0.27_65S-1_C_R_3.JPG  0.27  0.420040\n",
       "3        (1)_A_0.27_65S-1_C_R_4.JPG  0.27  0.302122\n",
       "4        (1)_A_0.27_65S-1_C_R_5.JPG  0.27  0.333441\n",
       "...                             ...   ...       ...\n",
       "2350  (99)_S_0.86_65S-197_M_R_1.jpg  0.86  0.508597\n",
       "2351  (99)_S_0.86_65S-197_M_R_2.jpg  0.86  0.655261\n",
       "2352  (99)_S_0.86_65S-197_M_R_3.jpg  0.86  0.662441\n",
       "2353  (99)_S_0.86_65S-197_M_R_4.jpg  0.86  0.635019\n",
       "2354  (99)_S_0.86_65S-197_M_R_5.jpg  0.86  0.594375\n",
       "\n",
       "[2355 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_new = pd.DataFrame()\n",
    "# df_new['pic_name'] = pic_names_new\n",
    "# df_new['OM'] = y_new\n",
    "# df_new['predict'] = yhat_new\n",
    "# df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
